{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df486ef2",
   "metadata": {},
   "source": [
    "## Finetune LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e57ab3e",
   "metadata": {},
   "source": [
    "### In this notebook you can find whole LLM finetuning process using Unsloth library for Nvidia GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0af79986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: OpenAI failed to import - ignoring for now.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, UnslothTrainingArguments, UnslothTrainer\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae9a47f",
   "metadata": {},
   "source": [
    "For results logging use wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f6b258d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlysun-pn\u001b[0m (\u001b[33mlysun-pn-ukrainian-catholic-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc86658a",
   "metadata": {},
   "source": [
    "Configuration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ab184a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "dataset_path = \"../../data/taras_shevchenko_poetry.jsonl\"\n",
    "output_dir = \"../../shevchenko_finetuned_lora\"\n",
    "lora_rank = 16\n",
    "lora_alpha = lora_rank # Scaling factor, often set equal to rank\n",
    "max_seq_length = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd59ee8",
   "metadata": {},
   "source": [
    "Load LLM from pretrained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d9003ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.3: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060 Ti. Num GPUs = 1. Max memory: 15.697 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=None, # Auto-detects bf16 on 4090/A100, fp16 on others\n",
    "    load_in_4bit=True, # Enable QLoRA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd9c68b",
   "metadata": {},
   "source": [
    "Apply unsloth LoRA adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffaa160f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.4.3 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=lora_rank,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=True, # Saves memory by recomputing activations\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9e7598",
   "metadata": {},
   "source": [
    "Load dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6841d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset into test and train splits\n",
    "dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "train_dataset, eval_dataset = dataset.train_test_split(test_size=0.1, seed=42, shuffle=True).values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2674c6",
   "metadata": {},
   "source": [
    "Add training arguments for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c2ed584",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = UnslothTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4, # Increase to simulate larger batch size if VRAM is limited\n",
    "    warmup_steps=10,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=5e-4,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=5, # Log metrics every 5 steps\n",
    "    optim=\"adamw_8bit\", # Memory-efficient optimizer\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    # --- Checkpointing ---\n",
    "    save_strategy=\"steps\", # Save checkpoints periodically\n",
    "    save_steps=50, # Save a checkpoint every 50 steps (adjust as needed)\n",
    "    save_total_limit=2, # Keep only the latest 2 checkpoints + the final one\n",
    "    # --- Evaluation (Optional) ---\n",
    "    eval_steps=50 if eval_dataset else None,\n",
    "    # --- Logging ---\n",
    "    report_to=[\"tensorboard\"], # Log to TensorBoard (or add \"wandb\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6d99ec",
   "metadata": {},
   "source": [
    "Init the trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1a9811e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1801754aba344a3e9607e96d93e546c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/207 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1015549b2d7a4ebf8a076670faac52c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/23 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "trainer = UnslothTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    args=training_args,\n",
    "    packing=False, # Set True if you want to pack multiple short sequences into one - saves compute but complex\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb0bf23",
   "metadata": {},
   "source": [
    "Train adapter using trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c98ed052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 207 | Num Epochs = 5 | Total steps = 30\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 39,976,960/7,000,000,000 (0.57% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 18:40, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.505900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.140300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.891700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.716400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.592800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.455200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30, training_loss=2.8837226231892905, metrics={'train_runtime': 1163.401, 'train_samples_per_second': 0.89, 'train_steps_per_second': 0.026, 'total_flos': 3.539777974566912e+16, 'train_loss': 2.8837226231892905})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6929ee57",
   "metadata": {},
   "source": [
    "Save the adapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "558ffee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final LoRA adapter saved to ../../shevchenko_finetuned_lora/final_adapter\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "final_adapter_path = os.path.join(output_dir, \"final_adapter\")\n",
    "model.save_pretrained(final_adapter_path)\n",
    "tokenizer.save_pretrained(final_adapter_path)\n",
    "print(f\"Final LoRA adapter saved to {final_adapter_path}\")\n",
    "\n",
    "# You can also save the full model if needed, but usually just the adapter is fine\n",
    "# model.save_pretrained_merged(\"shevchenko_full_model\", tokenizer, save_method = \"merged_16bit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70a6a052",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"User: –Ø–∫ —É–º—Ä—É, —Ç–æ –ø–æ—Ö–æ–≤–∞–π—Ç–µ \\n Agent: \"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "# add adapter \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36793c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: –Ø–∫ —É–º—Ä—É, —Ç–æ –ø–æ—Ö–æ–≤–∞–π—Ç–µ \n",
      " Agent:  –Ø–∫ —É–º—Ä—É, —Ç–æ –ø–æ—Ö–æ–≤–∞–π—Ç–µ,\n",
      "–ù—ñ, –Ω–µ –ø–æ—Ö–æ–≤–∞–π—Ç–µ,\n",
      "–ü–æ—Ö–æ–≤–∞–π—Ç–µ, –Ω–µ –ø–æ—Ö–æ–≤–∞–π—Ç–µ,\n",
      "–¢–æ –ø–æ—Ö–æ–≤–∞–π—Ç–µ –≤ –Ω–µ–≤–æ–ª—ñ.\n",
      "–Ü –Ω–µ —Ö–æ–¥—ñ—Ç–µ –ø–æ—Ö–∏–ª–∏—Ç–∏—Å—å,\n",
      "–©–æ–± –Ω–µ –≤–∏–π—à–ª–æ.\n",
      "–ü–æ—Ö–∏–ª–∏—Ç–µ—Å—å, –Ω–µ –ø–æ—Ö–∏–ª–∏—Ç–µ—Å—å,\n",
      "–©–æ–± –Ω–µ –≤–∏–π—à–ª–æ.\n",
      "–Ü –Ω–µ –≥–æ–≤–æ—Ä–∏—Ç–µ, —â–æ\n"
     ]
    }
   ],
   "source": [
    "# inference model and decoding\n",
    "outputs = model.generate(**inputs,\n",
    "                        #  do_sample=True, \n",
    "                        #  temperature=0.9, \n",
    "                        #  top_p=0.95, \n",
    "                        #  top_k=50,\n",
    "                        #  num_return_sequences=1,\n",
    "                         max_new_tokens=100,\n",
    "                        #  pad_token_id=tokenizer.eos_token_id\n",
    "                        )\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
